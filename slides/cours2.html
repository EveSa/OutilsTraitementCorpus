---
layout: presentation
title: Cours2
---
# Outils de Traitement de Corpus
cours 2
-vertical-
# Rappel du cours précédent

<h2 class="r-fit-text">📚📄Rappels sur les corpus📚📄</h2>
<h2 class="r-fit-text">🛠️Focus sur les tâches🛠️</h2>
<h2 class="r-fit-text">🔎Où trouver des corpus🔍</h2>

-vertical-

## Correction du TP

**Tâche** : Question Answering  
&rarr; Tâche de prédiction d'une réponse à partir d'une question

**Sous-tâche** : extractive-qa  
&rarr; Q&A à partir d'un contexte : le modèle ne doit extraire la réponse que du contexte qu'on lui donne

**Corpus choisi :** SQuAD : Stanford QUestion Answering Dataset

**Langues :** Anglais, uniquement  

-vertical-

&rarr; D'autres corpus existent dans d'autres langues, lesquels ?
- FQuAD en français
<img src="/img/extractive-qa-lang.PNG">

-vertical-

Le corpus comporte les informations suivantes : **id, title, context, question, answers**

**id :** Un champ unique pour distinguer une suite d'information

**title :** Le titre de la page wikipedia de laquelle est extrait le contexte

**context :** Une partie de la page wikipedia

**question :** Une question sur le contexte

**answers :** un dictionnaire qui contient le texte de la réponse et l'indice du mot qui débute de la réponse

-vertical-

**Modèles entraînés avec SQuAD :** Plus de 1805 modèles ont été entraînés à partir de SQuAD dont:

- distilbert
- dynamic_tinybert de Intel

**Taille :** 

- 98 169 lignes d'informations
- issu de 500+ article de wikipedia
- 16.3 MB

-vertical-

**Source :** 

Les contextes ont été récupérés à partir d'un projet existant : le **Nayuki Project** qui classe les pages de **wikipedia** avec un PageRank pour obtenir les articles les plus important de l'anglais.

Les questions ont été extraites par des travailleurs recrutés sur la plateforme Daemo sur une interface mise en place sur Amazon Mechanical Turks. Les travailleurs ont été recrutés au US et au Canada.

-vertical-

Les travailleurs devaient écrire et répondre à, au maximum, 5 questions par contexte. Les questions étaient entrées en champ libre et les réponses surlignées dans le texte (pour respecter l'extractivité de la tâche).

**Train-test split :** 

- 87599 pour le train

- 10570 pour le test

-vertical-

**Date de publication :** 

- sur hugging face : 2022
- l'article scientifique : 
  - 2016 
  - par Pranav Rajpurkar and Jian Zhang and Konstantin Lopyrev and Percy Liang
  - de l'université de **Stanford**
  - publié à EMNLP

-vertical-
# Cours 2

<h2 class="r-fit-text">🛍️Récupérer de nouvelles données🛍️</h2>
<h2 class="r-fit-text">⛽Rendre les données exploitable⛽</h2>
<h2 class="r-fit-text">🔧Les Outils pour explorer les données🔧</h2>

-horizontal-

## 🛍️Récupérer de nouvelles données🛍️

-vertical-

## Pourquoi ?

On parlait des RGPD : dans un cadre professionnel, on peut être ammené à avoir des données qu'il n'y a pas sur le web

Parfois les datasets pour certaines langues/tâches ne sont pas disponibles &rarr; il faut pourvoir se débrouiller

-vertical-

## 🕯️*Scrapper* le web🕯️

On veut récupérer les données **automatiquement**

(ex. SQuAD : 98 000 lignes d'informations &rarr; on ne veut pas aller les récupérer à la main)

-vertical-

## 🕯️*Scrapper* le web🕯️

<aside class="notes">
    Soit on vous donne des données déjà toutes prêtes:
    des données clients -> il faut faire des traitements Pour
        - anonymiser
        - nettoyer
        - : rendre exploitable
    Soit on vous dit juste ce dont vous avez besoin et il faut aller chercher les infos sur le net
</aside>

**Aller récupérer le contenu des sites**

- `Selenium`
<pre><code>> from selenium import webdriver</code></pre>
➡️ Une biliothèque pour automatiser les intéractions entre python et le navigateur
- `Requests` 
<pre><code>> import requests</code></pre>
➡️ La bibliothèque de base pour requêter le web depuis Python

-vertical-

## 🕯️*Scrapper* le web🕯️

**Récupérer le contenu textuel**

- `lxml`
<pre><code> from lxml import etree</code></pre>
➡️ La bibliothèque de base pour traiter du xml/html
- `Beautiful Soup`
<pre><code>
from bs4 import BeautifulSoup
</code></pre>
➡️ Une bibliothèque très répendue pour faire à peu près pareil que lxml
    
-vertical-

## Documents Structurés

Pour naviguer dans le contenu html, on utilise les concepts étudiés en cours de documents structurés et notamment **XPATH**

&rarr; [cheatsheet XPATH](https://devhints.io/xpath)

-vertical-

## Comment on fait ?

&rarr; Un [exemple](https://en.wikipedia.org/wiki/Entropy_(information_theory))

💡`f12` pour ouvrir l'inspecteur dynamique

💡`ctrl+U` pour ouvrir le code source de la page

<aside class="notes">
    dans un script
    on veut ouvrir la page avec requests
    copy xpath 
</aside>

-vertical-

<h2 class="r-fit-text">🔭Qu'est ce qu'on récupère ?🔭</h2>

<aside class="notes">
    On a vu la dernière fois qu'on pouvait avoir à peu près n'importe quoi comme nature de données de nos corpus
    Est ce que vous avez des idées de ce qui peut nous intéresser comme nature de données en NLP ?
</aside>

<ul>
    <li class="fragment">Des images <img src="https://upload.wikimedia.org/wikipedia/commons/b/b7/Manuscript_text_on_properties_of_the_mandrake_plant_Wellcome_L0037351.jpg" width="450" height="300"></li>
    <li class="fragment">Des sons 🔊</li>
    <li class="fragment">Des textes 🔡</li>
</ul>

-horizontal-

<aside class="notes">
    Comment exploiter ces données ?
    - Souvent pour faire de l'analyse sémantique, on passe par une représentation textuelle
    -> transformer l'oral en texte avec Whisper
    -> transformer les images en textes avec OCR
</aside>

## ⛽Rendre les données exploitable⛽

-vertical-

### 🖼️Convertir des images en texte🖼️

<div class="r-stack">
<div>
    <p>Les OCR</p>
<ul>
    <li>Avec Python :</li>
        <ul>
            <li>py-tesseract</li>
            <li>doctr</li>
        </ul>
    <li>Avec les modèles multimodaux ? :</li>
        <ul>
            <li>GPT-4</li>
            <li>Gemini</li>
        </ul>
</ul>
</div>

<img class="fragment" src="https://raw.githubusercontent.com/Yuliang-Liu/MultimodalOCR/main/images/GPT4V_Gemini.png">
</div>

-vertical-

### 🖼️Convertir des images en texte🖼️
#### Comment ça marche ❓

<img src="https://upload.wikimedia.org/wikipedia/commons/f/f7/MnistExamplesModified.png">

-vertical-

### 🔊Convertir les audio en texte🔊

Les modèles de speech-to-text
- Whisper
- Canary

-vertical-

### 🧼Convertir du texte en texte (propre)🧼
#### (Denoising Data)

- Comparer à un vocabulaire existant
    - (distance d'édition : Levenshtein)
- Utiliser des modèles (avec les mask token pred.)
- Utiliser des LLM

<aside class="notes">Problèmes des OOV : Out of Vocabulary</aside>

-vertical-

### 🏷️Annoter ses données🏷️

<aside class="notes">
    Si on cherche à faire de la classification, on va peut être devoir annoter ses données
    Soit on c'est de la classification de documents et on peut peut-être récupérer les info 
    au moment de la récupération
</aside>

**Au niveau des "mots"**

- Etiquetage Morphosyntaxique
- Entité Nommée

**Au niveau des documents**

- genre

-vertical-

### Les outils d'annotation

**Pour le POS Tagging**

Pour annoter à la main :
- CoNLL-U : un format

Pour annoter automatiquement :
- SpaCy : un outil automatique performant
- Stanza : l'outil de POS Tagging de Standford

<pre><code>
import stanza
import spacy_stanza
</code></pre>

-vertical-

### Les outils d'annotation

**Pour la reconnaissance d'entités nommées**

Pour annoter à la main :
- Glozz

Pour annoter avec des règles d'annotation :
- Unitex
- GATE
- SpaCy Matcher

Pour annoter automatiquement :
- SpaCy (again)

-vertical-

Le problème, c'est que pour annoter automatiquement, on a besoin de modèle.

Et pour avoir des modèles, on a besoin de données annotées...

Parfois la phase d'annotation manuelle ne peut pas être omise

-horizontal-

❓ Des questions ❓

💡 Des idées 💡

-vertical-

## Le point bonne pratique

Suivre les [conseils du MIT](https://mitcommlab.mit.edu/broad/commkit/file-structure/#BestPracticesFileStructures)

```
PROJECT/
├── bin/            <- compiled binaries. 
├── data/ 
│   ├── raw/
│   └── clean/
│
├── figures/        <- figures used in place of a "results" folder. 
├── scripts/
│   ├── process/    <- scripts to maniuplate data between raw, cleaned, final stages.
│   └── plot/	      <- intermediate plotting.
│
├── src
│   ├── model1/     <- various experimental models.
│   ├── model2/
│   └── model3/
│
├── LICENSE
├── Makefile
└── readme.md
```

-vertical-

<ol class="fade-out">
<li>Initialiser les dossiers de votre repo</li>
<ul>
    <li>réfléchissez aux traitements que l'on va effectuer pendant le cours : de quels dossiers vous allez avoir besoin</li>
</ul>
<li>Mettez en place un script python pour récupérer automatiquement des données depuis le web (quelques éléments suffiront)</li>
<li>Nettoyez ces données pour les présenter sous forme de texte</li>
<li>On veut obtenir les même colonnes que votre corpus de référence (le corpus de la semaine dernière)</li>
</ol>

-vertical-

<ul>
    <li>⚠️Ne récupérez que des données qu'il est légal de récupérer</li>
    <li>⚠️Vous n'avez pas à commit les fichiers récupérés dans votre repo &rarr; *push* des gros fichiers peut faire exploser votre git (cf. Le point bonne pratique de la semaine dernière)</li>
    <li>Votre code suffira pour l'évaluation</li>
</ul>

-vertical-

## Quelques précisions sur l'évaluation

- Le projet à rendre comprend:
    - l'analyse d'un corpus pré-existant
    - la constitution d'un corpus similaire à partir de données ouverte
    - l'applications de visualisation sur ces données
    - l'évaluation du corpus constitué

❗ Nous n'allons pas faire de Machine Learning sur le corpus ❗
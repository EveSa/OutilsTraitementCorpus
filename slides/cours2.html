---
layout: presentation
title: Cours2
---
# Outils de Traitement de Corpus
cours 2
-vertical-
# Rappel du cours prÃ©cÃ©dent

<h2 class="r-fit-text">ğŸ“šğŸ“„Rappels sur les corpusğŸ“šğŸ“„</h2>
<h2 class="r-fit-text">ğŸ› ï¸Focus sur les tÃ¢chesğŸ› ï¸</h2>
<h2 class="r-fit-text">ğŸ”OÃ¹ trouver des corpusğŸ”</h2>

-vertical-

## Correction du TP

**TÃ¢che** : Question Answering  
&rarr; TÃ¢che de prÃ©diction d'une rÃ©ponse Ã  partir d'une question

**Sous-tÃ¢che** : extractive-qa  
&rarr; Q&A Ã  partir d'un contexte : le modÃ¨le ne doit extraire la rÃ©ponse que du contexte qu'on lui donne

**Corpus choisi :** SQuAD : Stanford QUestion Answering Dataset

**Langues :** Anglais, uniquement  

-vertical-

&rarr; D'autres corpus existent dans d'autres langues, lesquels ?
- FQuAD en franÃ§ais
<img src="/img/extractive-qa-lang.PNG">

-vertical-

Le corpus comporte les informations suivantes : **id, title, context, question, answers**

**id :** Un champ unique pour distinguer une suite d'information

**title :** Le titre de la page wikipedia de laquelle est extrait le contexte

**context :** Une partie de la page wikipedia

**question :** Une question sur le contexte

**answers :** un dictionnaire qui contient le texte de la rÃ©ponse et l'indice du mot qui dÃ©bute de la rÃ©ponse

-vertical-

**ModÃ¨les entraÃ®nÃ©s avec SQuAD :** Plus de 1805 modÃ¨les ont Ã©tÃ© entraÃ®nÃ©s Ã  partir de SQuAD dont:

- distilbert
- dynamic_tinybert de Intel

**Taille :** 

- 98 169 lignes d'informations
- issu de 500+ article de wikipedia
- 16.3 MB

-vertical-

**Source :** 

Les contextes ont Ã©tÃ© rÃ©cupÃ©rÃ©s Ã  partir d'un projet existant : le **Nayuki Project** qui classe les pages de **wikipedia** avec un PageRank pour obtenir les articles les plus important de l'anglais.

Les questions ont Ã©tÃ© extraites par des travailleurs recrutÃ©s sur la plateforme Daemo sur une interface mise en place sur Amazon Mechanical Turks. Les travailleurs ont Ã©tÃ© recrutÃ©s au US et au Canada.

-vertical-

Les travailleurs devaient Ã©crire et rÃ©pondre Ã , au maximum, 5 questions par contexte. Les questions Ã©taient entrÃ©es en champ libre et les rÃ©ponses surlignÃ©es dans le texte (pour respecter l'extractivitÃ© de la tÃ¢che).

**Train-test split :** 

- 87599 pour le train

- 10570 pour le test

-vertical-

**Date de publication :** 

- sur hugging face : 2022
- l'article scientifique : 
  - 2016 
  - par Pranav Rajpurkar and Jian Zhang and Konstantin Lopyrev and Percy Liang
  - de l'universitÃ© de **Stanford**
  - publiÃ© Ã  EMNLP

-vertical-
# Cours 2

<h2 class="r-fit-text">ğŸ›ï¸RÃ©cupÃ©rer de nouvelles donnÃ©esğŸ›ï¸</h2>
<h2 class="r-fit-text">â›½Rendre les donnÃ©es exploitableâ›½</h2>
<h2 class="r-fit-text">ğŸ”§Les Outils pour explorer les donnÃ©esğŸ”§</h2>

-horizontal-

## ğŸ›ï¸RÃ©cupÃ©rer de nouvelles donnÃ©esğŸ›ï¸

-vertical-

## Pourquoi ?

On parlait des RGPD : dans un cadre professionnel, on peut Ãªtre ammenÃ© Ã  avoir des donnÃ©es qu'il n'y a pas sur le web

Parfois les datasets pour certaines langues/tÃ¢ches ne sont pas disponibles &rarr; il faut pourvoir se dÃ©brouiller

-vertical-

## ğŸ•¯ï¸*Scrapper* le webğŸ•¯ï¸

On veut rÃ©cupÃ©rer les donnÃ©es **automatiquement**

(ex. SQuAD : 98 000 lignes d'informations &rarr; on ne veut pas aller les rÃ©cupÃ©rer Ã  la main)

-vertical-

## ğŸ•¯ï¸*Scrapper* le webğŸ•¯ï¸

<aside class="notes">
    Soit on vous donne des donnÃ©es dÃ©jÃ  toutes prÃªtes:
    des donnÃ©es clients -> il faut faire des traitements Pour
        - anonymiser
        - nettoyer
        - : rendre exploitable
    Soit on vous dit juste ce dont vous avez besoin et il faut aller chercher les infos sur le net
</aside>

**Aller rÃ©cupÃ©rer le contenu des sites**

- `Selenium`
<pre><code>> from selenium import webdriver</code></pre>
â¡ï¸ Une biliothÃ¨que pour automatiser les intÃ©ractions entre python et le navigateur
- `Requests` 
<pre><code>> import requests</code></pre>
â¡ï¸ La bibliothÃ¨que de base pour requÃªter le web depuis Python

-vertical-

## ğŸ•¯ï¸*Scrapper* le webğŸ•¯ï¸

**RÃ©cupÃ©rer le contenu textuel**

- `lxml`
<pre><code> from lxml import etree</code></pre>
â¡ï¸ La bibliothÃ¨que de base pour traiter du xml/html
- `Beautiful Soup`
<pre><code>
from bs4 import BeautifulSoup
</code></pre>
â¡ï¸ Une bibliothÃ¨que trÃ¨s rÃ©pendue pour faire Ã  peu prÃ¨s pareil que lxml
    
-vertical-

## Documents StructurÃ©s

Pour naviguer dans le contenu html, on utilise les concepts Ã©tudiÃ©s en cours de documents structurÃ©s et notamment **XPATH**

&rarr; [cheatsheet XPATH](https://devhints.io/xpath)

-vertical-

## Comment on fait ?

&rarr; Un [exemple](https://en.wikipedia.org/wiki/Entropy_(information_theory))

ğŸ’¡`f12` pour ouvrir l'inspecteur dynamique

ğŸ’¡`ctrl+U` pour ouvrir le code source de la page

<aside class="notes">
    dans un script
    on veut ouvrir la page avec requests
    copy xpath 
</aside>

-vertical-

<h2 class="r-fit-text">ğŸ”­Qu'est ce qu'on rÃ©cupÃ¨re ?ğŸ”­</h2>

<aside class="notes">
    On a vu la derniÃ¨re fois qu'on pouvait avoir Ã  peu prÃ¨s n'importe quoi comme nature de donnÃ©es de nos corpus
    Est ce que vous avez des idÃ©es de ce qui peut nous intÃ©resser comme nature de donnÃ©es en NLP ?
</aside>

<ul>
    <li class="fragment">Des images <img src="https://upload.wikimedia.org/wikipedia/commons/b/b7/Manuscript_text_on_properties_of_the_mandrake_plant_Wellcome_L0037351.jpg" width="450" height="300"></li>
    <li class="fragment">Des sons ğŸ”Š</li>
    <li class="fragment">Des textes ğŸ”¡</li>
</ul>

-horizontal-

<aside class="notes">
    Comment exploiter ces donnÃ©es ?
    - Souvent pour faire de l'analyse sÃ©mantique, on passe par une reprÃ©sentation textuelle
    -> transformer l'oral en texte avec Whisper
    -> transformer les images en textes avec OCR
</aside>

## â›½Rendre les donnÃ©es exploitableâ›½

-vertical-

### ğŸ–¼ï¸Convertir des images en texteğŸ–¼ï¸

<div class="r-stack">
<div>
    <p>Les OCR</p>
<ul>
    <li>Avec Python :</li>
        <ul>
            <li>py-tesseract</li>
            <li>doctr</li>
        </ul>
    <li>Avec les modÃ¨les multimodaux ? :</li>
        <ul>
            <li>GPT-4</li>
            <li>Gemini</li>
        </ul>
</ul>
</div>

<img class="fragment" src="https://raw.githubusercontent.com/Yuliang-Liu/MultimodalOCR/main/images/GPT4V_Gemini.png">
</div>

-vertical-

### ğŸ–¼ï¸Convertir des images en texteğŸ–¼ï¸
#### Comment Ã§a marche â“

<img src="https://upload.wikimedia.org/wikipedia/commons/f/f7/MnistExamplesModified.png">

-vertical-

### ğŸ”ŠConvertir les audio en texteğŸ”Š

Les modÃ¨les de speech-to-text
- Whisper
- Canary

-vertical-

### ğŸ§¼Convertir du texte en texte (propre)ğŸ§¼
#### (Denoising Data)

- Comparer Ã  un vocabulaire existant
    - (distance d'Ã©dition : Levenshtein)
- Utiliser des modÃ¨les (avec les mask token pred.)
- Utiliser des LLM

<aside class="notes">ProblÃ¨mes des OOV : Out of Vocabulary</aside>

-vertical-

### ğŸ·ï¸Annoter ses donnÃ©esğŸ·ï¸

<aside class="notes">
    Si on cherche Ã  faire de la classification, on va peut Ãªtre devoir annoter ses donnÃ©es
    Soit on c'est de la classification de documents et on peut peut-Ãªtre rÃ©cupÃ©rer les info 
    au moment de la rÃ©cupÃ©ration
</aside>

**Au niveau des "mots"**

- Etiquetage Morphosyntaxique
- EntitÃ© NommÃ©e

**Au niveau des documents**

- genre

-vertical-

### Les outils d'annotation

**Pour le POS Tagging**

Pour annoter Ã  la main :
- CoNLL-U : un format

Pour annoter automatiquement :
- SpaCy : un outil automatique performant
- Stanza : l'outil de POS Tagging de Standford

<pre><code>
import stanza
import spacy_stanza
</code></pre>

-vertical-

### Les outils d'annotation

**Pour la reconnaissance d'entitÃ©s nommÃ©es**

Pour annoter Ã  la main :
- Glozz

Pour annoter avec des rÃ¨gles d'annotation :
- Unitex
- GATE
- SpaCy Matcher

Pour annoter automatiquement :
- SpaCy (again)

-vertical-

Le problÃ¨me, c'est que pour annoter automatiquement, on a besoin de modÃ¨le.

Et pour avoir des modÃ¨les, on a besoin de donnÃ©es annotÃ©es...

Parfois la phase d'annotation manuelle ne peut pas Ãªtre omise

-horizontal-

â“ Des questions â“

ğŸ’¡ Des idÃ©es ğŸ’¡

-vertical-

## Le point bonne pratique

Suivre les [conseils du MIT](https://mitcommlab.mit.edu/broad/commkit/file-structure/#BestPracticesFileStructures)

```
PROJECT/
â”œâ”€â”€ bin/            <- compiled binaries. 
â”œâ”€â”€ data/ 
â”‚   â”œâ”€â”€ raw/
â”‚   â””â”€â”€ clean/
â”‚
â”œâ”€â”€ figures/        <- figures used in place of a "results" folder. 
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ process/    <- scripts to maniuplate data between raw, cleaned, final stages.
â”‚   â””â”€â”€ plot/	      <- intermediate plotting.
â”‚
â”œâ”€â”€ src
â”‚   â”œâ”€â”€ model1/     <- various experimental models.
â”‚   â”œâ”€â”€ model2/
â”‚   â””â”€â”€ model3/
â”‚
â”œâ”€â”€ LICENSE
â”œâ”€â”€ Makefile
â””â”€â”€ readme.md
```

-vertical-

<ol class="fade-out">
<li>Initialiser les dossiers de votre repo</li>
<ul>
    <li>rÃ©flÃ©chissez aux traitements que l'on va effectuer pendant le cours : de quels dossiers vous allez avoir besoin</li>
</ul>
<li>Mettez en place un script python pour rÃ©cupÃ©rer automatiquement des donnÃ©es depuis le web (quelques Ã©lÃ©ments suffiront)</li>
<li>Nettoyez ces donnÃ©es pour les prÃ©senter sous forme de texte</li>
<li>On veut obtenir les mÃªme colonnes que votre corpus de rÃ©fÃ©rence (le corpus de la semaine derniÃ¨re)</li>
</ol>

-vertical-

<ul>
    <li>âš ï¸Ne rÃ©cupÃ©rez que des donnÃ©es qu'il est lÃ©gal de rÃ©cupÃ©rer</li>
    <li>âš ï¸Vous n'avez pas Ã  commit les fichiers rÃ©cupÃ©rÃ©s dans votre repo &rarr; *push* des gros fichiers peut faire exploser votre git (cf. Le point bonne pratique de la semaine derniÃ¨re)</li>
    <li>Votre code suffira pour l'Ã©valuation</li>
</ul>

-vertical-

## Quelques prÃ©cisions sur l'Ã©valuation

- Le projet Ã  rendre comprend:
    - l'analyse d'un corpus prÃ©-existant
    - la constitution d'un corpus similaire Ã  partir de donnÃ©es ouverte
    - l'applications de visualisation sur ces donnÃ©es
    - l'Ã©valuation du corpus constituÃ©

â— Nous n'allons pas faire de Machine Learning sur le corpus â—